{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Over & Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Boston Housing Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "In this project, you will use the Boston Housing Prices dataset to build several models to predict the prices of homes with particular qualities from the suburbs of Boston, MA.\n",
    "We will build models with several different parameters, which will change the goodness of fit for each. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Exploration\n",
    "Since we want to predict the value of houses, the **target variable**, `'MEDV'`, will be the variable we seek to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and explore the data. Clean the data for outliers and missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('../data/boston_data.csv')\n",
    "\n",
    "housing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of outliers\n",
    "\n",
    "Q1 = housing.describe().loc['25%']\n",
    "Q3 = housing.describe().loc['75%']\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Dropping all rows containing any values that are outside of the lower and upper bounds\n",
    "housing = housing[~((housing < lower_bound) | (housing > upper_bound)).any(axis = 1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we want to explore the data. Pick several varibables you think will be most correlated with the prices of homes in Boston, and create plots that show the data dispersion as well as the regression line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = housing.corr()\n",
    "\n",
    "mask = np.zeros_like(corr, dtype = bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(corr, mask = mask, annot = np.round(corr, 1), annot_kws = {\"size\": 8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most correlated variables:\n",
    "# INDUS (-0.5), NOX (-0.5), RM (0.6), AGE (-0.5), LSTAT (-0.7)\n",
    "\n",
    "sns.regplot(x = 'rm', y = 'medv', data = housing)\n",
    "plt.show()\n",
    "\n",
    "sns.regplot(x = 'lstat', y = 'medv', data = housing)\n",
    "plt.show()\n",
    "\n",
    "sns.regplot(x = 'ptratio', y = 'medv', data = housing)\n",
    "plt.show()\n",
    "\n",
    "sns.regplot(x = 'age', y = 'medv', data = housing)\n",
    "plt.show()\n",
    "\n",
    "sns.regplot(x = 'lstat', y = 'medv', data = housing)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do these plots tell you about the relationships between these variables and the prices of homes in Boston? Are these the relationships you expected to see in these variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These relationships are what we would expect to see based on previous research and common sense: areas with higher non-retail business acreage, higher nitric oxides concentration, lower average number of rooms, higher proportion of older homes, and higher proportion of lower-income residents are generally less desirable and therefore have lower housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a heatmap of the remaining variables. Are there any variables that you did not consider that have very high correlations? What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_vars = housing[[var for var in housing.columns if var not in ['indus', 'nox', 'rm', 'age', 'lstat']]]\n",
    "\n",
    "remaining_corr = remaining_vars.corr()\n",
    "\n",
    "mask = np.zeros_like(remaining_corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(remaining_corr, annot = True, mask = mask, fmt = '.1f')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# There are no other variables that have a very high correlation with MEDV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Statistics\n",
    "Calculate descriptive statistics for housing price. Include the minimum, maximum, mean, median, and standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we've taken care of the outliers (also those in MEDV)\n",
    "\n",
    "medv = housing['medv']\n",
    "\n",
    "min_medv = medv.min()\n",
    "max_medv = medv.max()\n",
    "mean_medv = medv.mean()\n",
    "median_medv = medv.median()\n",
    "std_medv = medv.std()\n",
    "\n",
    "print('Minimum housing price:', min_medv)\n",
    "print('Maximum housing price:', max_medv)\n",
    "print('Mean housing price:', mean_medv)\n",
    "print('Median housing price:', median_medv)\n",
    "print('Standard deviation of housing price:', std_medv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Developing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Define a Performance Metric\n",
    "What is the performance meteric with which you will determine the performance of your model? Create a function that calculates this performance metric, and then returns the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def performance_metric(y_true, y_predict):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "    return r2_score(y_true, y_predict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Shuffle and Split Data\n",
    "Split the data into the testing and training datasets. Shuffle the data as well to remove any bias in selecting the training and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing.drop('medv', axis = 1)\n",
    "y = housing['medv']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, train_size = 0.8)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state = 0)\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Analyzing Model Performance\n",
    "Next, we are going to build a Random Forest Regressor, and test its performance with several different parameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "Lets build the different models. Set the max_depth parameter to 2, 4, 6, 8, and 10 respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_depth_2 = RandomForestRegressor(max_depth = 2)\n",
    "rf_depth_4 = RandomForestRegressor(max_depth = 4)\n",
    "rf_depth_6 = RandomForestRegressor(max_depth = 6)\n",
    "rf_depth_8 = RandomForestRegressor(max_depth = 8)\n",
    "rf_depth_10 = RandomForestRegressor(max_depth = 10)\n",
    "\n",
    "forests = [rf_depth_2, rf_depth_4, rf_depth_6, rf_depth_8, rf_depth_10]\n",
    "\n",
    "[rf.fit(X_train, y_train) for rf in forests]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the score for each tree on the training set and on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_scores = [performance_metric(y_train, rf.predict(X_train)) for rf in forests]\n",
    "testing_scores = [performance_metric(y_test, rf.predict(X_test)) for rf in forests]\n",
    "\n",
    "plt.plot(training_scores, label = 'Training scores')\n",
    "plt.plot(testing_scores, label = 'Testing scores')\n",
    "\n",
    "plt.xticks([0, 1, 2, 3, 4], [2, 4, 6, 8, 10])\n",
    "plt.xlabel('Depth of trees')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these results tell you about the effect of the depth of the trees on the performance of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The performance on the testing data initially improves and then decreases after a certain point.\n",
    "# This indicates that the model is overfitting to the training data when the depth of the trees becomes too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff\n",
    "When the model is trained with a maximum depth of 1, does the model suffer from high bias or from high variance? How about when the model is trained with a maximum depth of 10? Check out this article before answering: https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the model is trained with a maximum depth of 1, it suffers from high bias.\n",
    "# When the model is trained with a maximum depth of 10, it suffers from high variance.\n",
    "# To achieve good performance, the goal is to find a model that strikes a balance between these two characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best-Guess Optimal Model\n",
    "What is the max_depth parameter that you think would optimize the model? Run your model and explain its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimal value of the max_depth parameter is likely to be somewhere between 4 and 6.\n",
    "# The training and testing scores are both relatively high and there is not a significant gap between the two.\n",
    "\n",
    "rf_depth_5 = RandomForestRegressor(max_depth = 5)\n",
    "rf_depth_5.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_depth_5.predict(X_test)\n",
    "\n",
    "performance_metric(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicability\n",
    "*In a few sentences, discuss whether the constructed model should or should not be used in a real-world setting.*  \n",
    "**Hint:** Some questions to answering:\n",
    "- *How relevant today is data that was collected from 1978?*\n",
    "- *Are the features present in the data sufficient to describe a home?*\n",
    "- *Is the model robust enough to make consistent predictions?*\n",
    "- *Would data collected in an urban city like Boston be applicable in a rural city?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model constructed from 1978 data may not be relevant today as real estate prices and factors affecting them have likely changed over time.\n",
    "# The features used to describe a home in the model may not be sufficient to accurately represent a home in a real-world setting, as there may be other important factors to consider.\n",
    "# The model's performance, as indicated by the R^2 score, may not be robust enough to make consistent predictions.\n",
    "# Real estate prices in urban cities like Boston may not be applicable to a rural city, as the factors affecting real estate prices may differ greatly between the two locations."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
